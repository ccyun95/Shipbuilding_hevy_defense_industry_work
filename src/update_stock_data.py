import argparse
import logging
import os
import json
from pathlib import Path
from datetime import datetime, timedelta
from dateutil import tz
import time
import pandas as pd
from pykrx import stock

# =========================
# ì„¤ì •
# =========================
DATA_DIR = Path(os.getenv("GITHUB_WORKSPACE", ".")) / "data"
OUTPUT_SUFFIX = "_stock_data.csv"
ENCODING = "utf-8-sig"     # ì—‘ì…€ í˜¸í™˜
SLEEP_SEC = 0.3            # API ê³¼í˜¸ì¶œ ë°©ì§€
WINDOW_DAYS_INIT = 370     # ì‹ ê·œ ìƒì„± ì‹œ ê³¼ê±° 1ë…„+Î±
BACKFILL_CAL_DAYS_FOR_SHORT = 10  # ê³µë§¤ë„ì”ê³ /ë¹„ì¤‘ ì§€ì—° ê³µê°œ ë³´ì •ìš© ìµœì†Œ ì¬ìˆ˜ì§‘ êµ¬ê°„(ìº˜ë¦°ë” ì¼ìˆ˜)

REQ_COLS = [
    "ì¼ì","ì‹œê°€","ê³ ê°€","ì €ê°€","ì¢…ê°€","ê±°ë˜ëŸ‰","ë“±ë½ë¥ ",
    "ê¸°ê´€ í•©ê³„","ê¸°íƒ€ë²•ì¸","ê°œì¸","ì™¸êµ­ì¸ í•©ê³„","ì „ì²´",
    "ê³µë§¤ë„","ê³µë§¤ë„ë¹„ì¤‘","ê³µë§¤ë„ì”ê³ ","ê³µë§¤ë„ì”ê³ ë¹„ì¤‘"
]

KST = tz.gettz("Asia/Seoul")

# pykrx ë‚´ë¶€ ë¡œê·¸ ë¬µìŒ
for name in ["pykrx", "pykrx.website", "pykrx.website.comm", "pykrx.website.comm.util"]:
    logging.getLogger(name).disabled = True

# =========================
# ìœ í‹¸
# =========================
def kst_today_date():
    return datetime.now(tz=KST).date()

def yyyymmdd(d):
    return d.strftime("%Y%m%d")

def empty_with_cols(cols):
    data = {}
    for c in cols:
        data[c] = pd.Series(dtype="object") if c == "ì¼ì" else pd.Series(dtype="float64")
    return pd.DataFrame(data)

def read_company_list(path: Path):
    rows = []
    if not path.exists():
        raise FileNotFoundError(f"ê¸°ì—… ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {path}")
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if "," in line:
                name, ticker = [x.strip() for x in line.split(",", 1)]
            else:
                parts = line.replace("\t", " ").split()
                if len(parts) < 2:
                    logging.warning("ê¸°ì—… ë¼ì¸ íŒŒì‹± ë¶ˆê°€: %s", line)
                    continue
                name, ticker = parts[0], parts[1]
            rows.append((name, ticker.zfill(6)))
    return rows

def last_trading_day_by_ohlcv(ticker: str, today):
    start = today - timedelta(days=30)
    df = stock.get_market_ohlcv(yyyymmdd(start), yyyymmdd(today), ticker)
    if df is None or df.empty:
        start = today - timedelta(days=90)
        df = stock.get_market_ohlcv(yyyymmdd(start), yyyymmdd(today), ticker)
    if df is None or df.empty:
        raise RuntimeError(f"{ticker}: ìµœê·¼ ê±°ë˜ ìë£Œ ì—†ìŒ")
    return pd.to_datetime(df.index.max()).date()

def normalize_date_index(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return empty_with_cols(["ì¼ì"])
    df = df.copy()
    if df.index.name is None:
        df.index.name = "ì¼ì"
    idx = pd.to_datetime(df.index, errors="coerce")
    df.index = idx
    df.reset_index(inplace=True)
    df.rename(columns={df.columns[0]: "ì¼ì"}, inplace=True)
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"], errors="coerce").dt.strftime("%Y-%m-%d")
    return df

def _normalize_date_col(df: pd.DataFrame) -> pd.DataFrame:
    """CSV/ìˆ˜ì§‘ ë°ì´í„° ëª¨ë‘ 'ì¼ì'ë¥¼ YYYY-MM-DD ë¬¸ìì—´ë¡œ í‘œì¤€í™”."""
    if df is None or df.empty or "ì¼ì" not in df.columns:
        return df
    df = df.copy()
    df["ì¼ì"] = pd.to_datetime(df["ì¼ì"], errors="coerce").dt.strftime("%Y-%m-%d")
    return df

def _to_float_clean(s):
    """ë¬¸ì í˜•íƒœ ìˆ˜ì¹˜ë¥¼ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜: ì‰¼í‘œ/ê³µë°±/% ì œê±°"""
    try:
        if pd.isna(s):
            return 0.0
        x = str(s).strip()
        if x.endswith("%"):
            x = x[:-1]
        x = x.replace(",", "").replace(" ", "")
        return float(x)
    except Exception:
        return 0.0

def _pick_first_col(cols, candidates):
    """colsì—ì„œ candidates(ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸) ì¤‘ ì²˜ìŒìœ¼ë¡œ ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ëª… ë°˜í™˜"""
    for key in candidates:
        for c in cols:
            if key in c:
                return c
    return None

def rename_investor_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty or "ì¼ì" not in df.columns:
        return empty_with_cols(["ì¼ì","ê¸°ê´€ í•©ê³„","ê¸°íƒ€ë²•ì¸","ê°œì¸","ì™¸êµ­ì¸ í•©ê³„","ì „ì²´"])
    mapping = {
        "ê¸°ê´€í•©ê³„":"ê¸°ê´€ í•©ê³„", "ì™¸êµ­ì¸í•©ê³„":"ì™¸êµ­ì¸ í•©ê³„",
        "ê¸°ê´€ í•©ê³„":"ê¸°ê´€ í•©ê³„", "ì™¸êµ­ì¸ í•©ê³„":"ì™¸êµ­ì¸ í•©ê³„",
        "ê°œì¸":"ê°œì¸", "ê¸°íƒ€ë²•ì¸":"ê¸°íƒ€ë²•ì¸", "ì „ì²´":"ì „ì²´"
    }
    df = df.rename(columns={c: mapping.get(c, c) for c in df.columns})
    for need in ["ê¸°ê´€ í•©ê³„","ê¸°íƒ€ë²•ì¸","ê°œì¸","ì™¸êµ­ì¸ í•©ê³„","ì „ì²´"]:
        if need not in df.columns:
            df[need] = 0
    return df[["ì¼ì","ê¸°ê´€ í•©ê³„","ê¸°íƒ€ë²•ì¸","ê°œì¸","ì™¸êµ­ì¸ í•©ê³„","ì „ì²´"]]

def rename_short_cols(df: pd.DataFrame, is_balance=False) -> pd.DataFrame:
    """
    ê³µë§¤ë„ ê´€ë ¨ í‘œì¤€í™”.
    - is_balance=False: ê±°ë˜(ë³¼ë¥¨) â†’ ['ê³µë§¤ë„','ê³µë§¤ë„ë¹„ì¤‘']
    - is_balance=True : ì”ê³      â†’ ['ê³µë§¤ë„ì”ê³ ','ê³µë§¤ë„ì”ê³ ë¹„ì¤‘']
    â€» í¼ì„¼íŠ¸/ì‰¼í‘œ ë“± ë¬¸ìì—´ ì „ì²˜ë¦¬ í¬í•¨
    """
    if df is None or df.empty or "ì¼ì" not in df.columns:
        base = ["ê³µë§¤ë„ì”ê³ ","ê³µë§¤ë„ì”ê³ ë¹„ì¤‘"] if is_balance else ["ê³µë§¤ë„","ê³µë§¤ë„ë¹„ì¤‘"]
        return empty_with_cols(["ì¼ì"] + base)

    dfc = df.copy()

    if is_balance:
        # pykrx ì”ê³  ê³„ì—´ì—ì„œ í”í•œ ì»¬ëŸ¼ë“¤: 'ê³µë§¤ë„ì”ê³ ìˆ˜ëŸ‰/ê¸ˆì•¡', 'ì”ê³ ìˆ˜ëŸ‰/ê¸ˆì•¡', 'ê³µë§¤ë„ì”ê³ ë¹„ì¤‘'('ì”ê³ ë¹„ì¤‘')
        amt_col = _pick_first_col(
            dfc.columns,
            ["ê³µë§¤ë„ì”ê³ ", "ì”ê³ ìˆ˜ëŸ‰", "ì”ê³ ê¸ˆì•¡", "ì”ê³ ", "BAL_QTY", "BAL_AMT"]
        )
        rto_col = _pick_first_col(
            dfc.columns,
            ["ê³µë§¤ë„ì”ê³ ë¹„ì¤‘", "ì”ê³ ë¹„ì¤‘", "BAL_RTO", "ë¹„ì¤‘"]  # 'ë¹„ì¤‘'ì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆì–´ë„ ìš°ì„ ìˆœìœ„ìƒ ë’¤ë¡œ ë‘ 
        )

        dfc["ê³µë§¤ë„ì”ê³ "] = dfc[amt_col].apply(_to_float_clean) if amt_col else 0.0
        dfc["ê³µë§¤ë„ì”ê³ ë¹„ì¤‘"] = dfc[rto_col].apply(_to_float_clean) if rto_col else 0.0

        keep = ["ì¼ì","ê³µë§¤ë„ì”ê³ ","ê³µë§¤ë„ì”ê³ ë¹„ì¤‘"]
        out = dfc[keep].copy()

    else:
        # ê±°ë˜(ë³¼ë¥¨) ê³„ì—´: 'ê³µë§¤ë„ê±°ë˜ëŸ‰/ëŒ€ê¸ˆ', 'ê³µë§¤ë„ë¹„ì¤‘'
        amt_col = _pick_first_col(
            dfc.columns,
            ["ê³µë§¤ë„ê±°ë˜ëŸ‰", "ê³µë§¤ë„", "ê±°ë˜ëŸ‰", "SV_QTY", "SV_AMT"]
        )
        rto_col = _pick_first_col(
            dfc.columns,
            ["ê³µë§¤ë„ë¹„ì¤‘", "ë¹„ì¤‘", "SV_RTO"]
        )

        dfc["ê³µë§¤ë„"] = dfc[amt_col].apply(_to_float_clean) if amt_col else 0.0
        dfc["ê³µë§¤ë„ë¹„ì¤‘"] = dfc[rto_col].apply(_to_float_clean) if rto_col else 0.0

        keep = ["ì¼ì","ê³µë§¤ë„","ê³µë§¤ë„ë¹„ì¤‘"]
        out = dfc[keep].copy()

    # ë‚ ì§œ í‘œì¤€í™”
    out["ì¼ì"] = pd.to_datetime(out["ì¼ì"], errors="coerce").dt.strftime("%Y-%m-%d")
    return out

def ensure_all_cols(df: pd.DataFrame) -> pd.DataFrame:
    for col in REQ_COLS:
        if col not in df.columns:
            df[col] = 0
    return df[REQ_COLS]

# ---------- CSV íŒŒì¼ëª… ê·œì¹™: <ì´ë¦„>_<6ìë¦¬í‹°ì»¤>_stock_data.csv ----------
def csv_path_for(eng_name: str, ticker: str) -> Path:
    return DATA_DIR / f"{eng_name}_{str(ticker).zfill(6)}{OUTPUT_SUFFIX}"

def fetch_block(ticker: str, start_d, end_d) -> pd.DataFrame:
    s, e = yyyymmdd(start_d), yyyymmdd(end_d)
    ohlcv = stock.get_market_ohlcv(s, e, ticker)
    df1 = normalize_date_index(ohlcv)

    inv = stock.get_market_trading_volume_by_date(s, e, ticker)
    df2 = rename_investor_cols(normalize_date_index(inv))

    try:
        sv = stock.get_shorting_volume_by_date(s, e, ticker)
    except Exception:
        sv = pd.DataFrame()
    df3 = rename_short_cols(normalize_date_index(sv), is_balance=False)

    try:
        sb = stock.get_shorting_balance_by_date(s, e, ticker)
    except Exception:
        sb = pd.DataFrame()
    df4 = rename_short_cols(normalize_date_index(sb), is_balance=True)

    df = df1.merge(df2, on="ì¼ì", how="left") \
            .merge(df3, on="ì¼ì", how="left") \
            .merge(df4, on="ì¼ì", how="left")

    # ìˆ«ì ë³€í™˜(í¼ì„¼íŠ¸/ì‰¼í‘œ ì •ê·œí™”ëŠ” ê° rename_*ì—ì„œ ì²˜ë¦¬ ì™„ë£Œ)
    df = ensure_all_cols(df)
    for c in [c for c in df.columns if c != "ì¼ì"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

    return df.sort_values("ì¼ì", ascending=False)

# =========================
# (ìœ ì§€) TÂ·T-1ì˜ ê³µë§¤ë„ì”ê³ /ë¹„ì¤‘ì„ T-2 ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
# =========================
def propagate_short_balance_from_t2(df: pd.DataFrame) -> pd.DataFrame:
    """
    ìµœì‹  ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê¸°ì¤€ìœ¼ë¡œ,
    - 2í–‰(2ê±°ë˜ì¼ ì „)ì˜ 'ê³µë§¤ë„ì”ê³ /ê³µë§¤ë„ì”ê³ ë¹„ì¤‘' ê°’ì„ ì½ì–´,
    - 0í–‰(í˜„ê±°ë˜ì¼), 1í–‰(ì „ì¼)ì˜ ë‘ ì»¬ëŸ¼ì„ ë™ì¼ ê°’ìœ¼ë¡œ ë®ì–´ì“´ë‹¤.
    - 2ê±°ë˜ì¼ ì „ ë°ì´í„°ê°€ ì—†ìœ¼ë©´(í–‰<3) ë³€ê²½í•˜ì§€ ì•ŠìŒ.
    """
    cols = ["ê³µë§¤ë„ì”ê³ ", "ê³µë§¤ë„ì”ê³ ë¹„ì¤‘"]
    if df is None or df.empty or not all(c in df.columns for c in cols):
        return df
    df = df.copy()
    try:
        df["__dt__"] = pd.to_datetime(df["ì¼ì"], errors="coerce")
        df.sort_values("__dt__", ascending=False, inplace=True)
        df.drop(columns="__dt__", inplace=True)
    except Exception:
        df.sort_values("ì¼ì", ascending=False, inplace=True)

    if len(df) >= 3:
        ref = df.iloc[2][cols].values
        for idx in [0, 1]:
            df.iloc[idx, df.columns.get_indexer(cols)] = ref
    return df

# =========================
# íšŒì‚¬ë³„ ì—…ë°ì´íŠ¸
# =========================
def upsert_company(eng_name: str, ticker: str, run_on_holiday: bool):
    out_path = csv_path_for(eng_name, ticker)
    today = kst_today_date()
    end_date = last_trading_day_by_ohlcv(ticker, today)

    # ---- ë°±í•„ ìœˆë„ìš° ì ìš©: ìµœê·¼ Nì¼ + last_have - 2ì¼ê¹Œì§€ í›„í‡´ ----
    if out_path.exists():
        base = pd.read_csv(out_path, encoding=ENCODING)
        base = _normalize_date_col(base)
        last_have = None if base.empty else pd.to_datetime(base["ì¼ì"], errors="coerce").dt.date.max()

        start_date_base = (last_have + timedelta(days=1)) if last_have else (end_date - timedelta(days=WINDOW_DAYS_INIT))
        backfill_floor = end_date - timedelta(days=BACKFILL_CAL_DAYS_FOR_SHORT)
        if last_have:
            conservative_floor = last_have - timedelta(days=2)
            backfill_floor = min(backfill_floor, conservative_floor)

        start_date = min(start_date_base, backfill_floor)
    else:
        start_date = end_date - timedelta(days=WINDOW_DAYS_INIT)

    if (end_date < today) and (not run_on_holiday) and (not out_path.exists()):
        logging.info("[%s] íœ´ì¥ì¼(run_on_holiday=False) â†’ ì‹ ê·œ ìƒì„± ìŠ¤í‚µ", eng_name)
        return False

    if start_date > end_date:
        logging.info("[%s] ìµœì‹  ìƒíƒœ (ì¶”ê°€ ë°ì´í„° ì—†ìŒ)", eng_name)
        return False

    logging.info("[%s] ì¬ìˆ˜ì§‘ êµ¬ê°„: %s ~ %s (í‹°ì»¤ %s)", eng_name, start_date, end_date, ticker)
    df = fetch_block(ticker, start_date, end_date)
    df = _normalize_date_col(df)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    if out_path.exists():
        base = pd.read_csv(out_path, encoding=ENCODING)
        base = _normalize_date_col(base)

        # ë³‘í•©: base(ìš°ì„ ìˆœìœ„ ë‚®ìŒ) + df(ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        base["__pri__"] = 0
        df["__pri__"] = 1
        merged = pd.concat([base, df], ignore_index=True)

        # ìµœì‹ â†’ê³¼ê±°, ê°™ì€ ì¼ìëŠ” __pri__ê°€ ë†’ì€(df) ê°’ì´ ë¨¼ì € ì˜¤ë„ë¡
        merged["__dt__"] = pd.to_datetime(merged["ì¼ì"], errors="coerce")
        merged.sort_values(["__dt__", "__pri__"], ascending=[False, False], inplace=True, kind="mergesort")

        # ë™ì¼ 'ì¼ì' ì¤‘ë³µ ì œê±°: ì²« í–‰(=ê°€ì¥ ìµœì‹  & df ìš°ì„ )ì´ ë‚¨ê²Œ
        merged.drop_duplicates(subset=["ì¼ì"], keep="first", inplace=True)
        merged.drop(columns=["__dt__", "__pri__"], inplace=True)
        merged.reset_index(drop=True, inplace=True)

        # TÂ·T-1 â† T-2 ê°’ ë®ì–´ì“°ê¸°
        merged = propagate_short_balance_from_t2(merged)

        # ìµœì¢… ì •ë ¬ ë° ì €ì¥
        merged["__dt__"] = pd.to_datetime(merged["ì¼ì"], errors="coerce")
        merged.sort_values("__dt__", ascending=False, inplace=True)
        merged.drop(columns="__dt__", inplace=True)
        merged.to_csv(out_path, index=False, encoding=ENCODING, lineterminator="\n")
        logging.info("[%s] ì—…ë°ì´íŠ¸ â†’ %s (ì´ %dí–‰)", eng_name, out_path, len(merged))
    else:
        df = propagate_short_balance_from_t2(df)
        df.to_csv(out_path, index=False, encoding=ENCODING, lineterminator="\n")
        logging.info("[%s] ì‹ ê·œ ìƒì„± â†’ %s (ì´ %dí–‰)", eng_name, out_path, len(df))
    return True

# =========================
# ê¸°ì—…ë³„ JSON + index.html ìƒì„±
#  - ë‹¨ì¼ index.json ìƒì„± ì—†ìŒ
# =========================
def emit_per_ticker_json(companies, rows_limit=None):
    api_dir = Path(os.getenv("GITHUB_WORKSPACE", ".")) / "docs" / "api"
    api_dir.mkdir(parents=True, exist_ok=True)
    cnt = 0
    for name, ticker in companies:
        csv_path = csv_path_for(name, ticker)
        if not csv_path.exists():
            continue
        try:
            df = pd.read_csv(csv_path, encoding=ENCODING)
        except Exception:
            df = pd.read_csv(csv_path)
        if df.empty:
            continue
        if rows_limit:
            df = df.head(int(rows_limit))

        item = {
            "name": name,
            "ticker": str(ticker).zfill(6),
            "columns": [str(c) for c in df.columns],
            "rows": df.astype(str).values.tolist(),
            "row_count": int(len(df)),
        }
        out = api_dir / f"{name}_{str(ticker).zfill(6)}.json"
        out.write_text(json.dumps(item, ensure_ascii=False, separators=(",", ":")), encoding="utf-8")
        cnt += 1
    logging.info("ê¸°ì—…ë³„ JSON ìƒì„±: %dê°œ", cnt)

def emit_index_html(companies, rows_limit=None):
    import html as _html
    from string import Template  # â† f-string ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì œ ë°©ì§€ë¥¼ ìœ„í•´ Template ì‚¬ìš©
    docs_dir = Path(os.getenv("GITHUB_WORKSPACE", ".")) / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)

    sections = []
    generated = datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M:%S %Z")

    for name, ticker in companies:
        csv_path = csv_path_for(name, ticker)
        if not csv_path.exists():
            continue
        try:
            df = pd.read_csv(csv_path, encoding=ENCODING)
        except Exception:
            df = pd.read_csv(csv_path)
        if df.empty:
            continue
        if rows_limit:
            df = df.head(int(rows_limit))

        columns = [str(c) for c in df.columns]
        rows = df.astype(str).values.tolist()

        thead = "".join(f"<th>{_html.escape(c)}</th>" for c in columns)
        tbody = "\n".join(
            "<tr>" + "".join(f"<td>{_html.escape(v)}</td>" for v in row) + "</tr>" for row in rows
        )
        sec_id = f"{name}_{str(ticker).zfill(6)}"

        # ğŸ”¹ ì°¨íŠ¸ìš© ì„¹ì…˜ë³„ inline JSON (CORS íšŒí”¼, ê¸°ì¡´ êµ¬ì¡°/ì£¼ì„ ìœ ì§€)
        payload = {
            "name": name,
            "ticker": str(ticker).zfill(6),
            "columns": [str(c).strip() for c in columns],  # ì•ˆì „: ì»¬ëŸ¼ëª… trim
            "rows": rows,  # ë¬¸ìì—´ ê·¸ëŒ€ë¡œ(í‘œì™€ ë™ì¼ ì†ŒìŠ¤)
        }
        json_raw = json.dumps(payload, ensure_ascii=False)
        json_safe = json_raw.replace("</", "<\\/")  # </script> ì°¨ë‹¨

        # ğŸ”¹ í‘œ + ì°¨íŠ¸ 2ê°œ(ì„¸ë¡œ ìŠ¤íƒ) + ì„¹ì…˜ë³„ ë°ì´í„° ìŠ¤í¬ë¦½íŠ¸
        sections.append(f"""
<section id="{_html.escape(sec_id)}">
  <h2>{_html.escape(name)} ({str(ticker).zfill(6)})</h2>
  <div class="scroll">
    <table>
      <thead><tr>{thead}</tr></thead>
      <tbody>
      {tbody}
      </tbody>
    </table>
  </div>
  <p class="meta">rows: {len(rows)} Â· source: data/{_html.escape(csv_path.name)} Â· json: api/{_html.escape(sec_id)}.json</p>

  <div class="charts">
    <div id="chart-price-{_html.escape(sec_id)}" class="chart"></div>
    <div id="chart-flow-{_html.escape(sec_id)}" class="chart"></div>
  </div>

  <script id="data-{_html.escape(sec_id)}" type="application/json">{json_safe}</script>
</section>""")

    def _id_from(sec_html: str) -> str:
        try:
            return sec_html.split('id="', 1)[1].split('"', 1)[0]
        except Exception:
            return "section"

    nav = "".join(f'<a href="#{_id_from(s)}">{_id_from(s)}</a>' for s in sections)

    # ğŸ”¹ 2ê°œ ì°¨íŠ¸(ê°€ê²©/ì§€í‘œ, ìˆ˜ê¸‰/ê³µë§¤ë„)ë¥¼ ê·¸ë¦¬ëŠ” ìŠ¤í¬ë¦½íŠ¸ í¬í•¨
    html_template = Template("""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>KRX ê¸°ì—…ë³„ ë°ì´í„° í…Œì´ë¸”</title>
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<style>
  body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }
  header { margin-bottom: 20px; }
  .meta-top { color:#666; font-size:14px; }
  .nav { display:flex; flex-wrap:wrap; gap:8px 16px; margin-top:8px; }
  .nav a { font-size:13px; text-decoration:none; color:#2563eb; }
  section { margin: 32px 0; }
  h2 { font-size: 18px; margin: 12px 0; }
  .scroll { overflow:auto; max-height: 60vh; border:1px solid #e5e7eb; }
  table { border-collapse: collapse; width: 100%; font-size: 13px; }
  th, td { border: 1px solid #e5e7eb; padding: 6px 8px; text-align: right; }
  th:first-child, td:first-child { text-align: left; white-space: nowrap; }
  thead th { position: sticky; top:0; background:#fafafa; }
  .meta { color:#666; font-size:12px; }
  .charts { width: 100%; display: flex; flex-direction: column; gap: 12px; margin-top: 12px; }
  .chart { width: 100%; height: 560px; border:1px solid #e5e7eb; }
</style>
</head>
<body>
<header>
  <h1>KRX ê¸°ì—…ë³„ ë°ì´í„° í…Œì´ë¸”</h1>
  <div class="meta-top">ìƒì„± ì‹œê°: $generated Â· íƒ€ì„ì¡´: Asia/Seoul</div>
  <nav class="nav">$nav</nav>
</header>

$sections

<script>
// ===== ìœ í‹¸ =====
function SMA(arr,n){const o=Array(arr.length).fill(null);let s=0,q=[];for(let i=0;i<arr.length;i++){const v=+arr[i]||0;q.push(v);s+=v;if(q.length>n)s-=q.shift();if(q.length===n)o[i]=s/n}return o}
function EMA(arr,n){const o=Array(arr.length).fill(null);const k=2/(n+1);let p=null;for(let i=0;i<arr.length;i++){const v=+arr[i]||0;p=(p==null)?v:v*k+p*(1-k);o[i]=p}return o}
function STD(arr,n){const o=Array(arr.length).fill(null);let q=[];for(let i=0;i<arr.length;i++){const v=+arr[i]||0;q.push(v);if(q.length>n)q.shift();if(q.length===n){const m=q.reduce((a,b)=>a+b,0)/n;const s2=q.reduce((a,b)=>a+(b-m)*(b-m),0)/n;o[i]=Math.sqrt(s2)}}return o}
function RSI(close,n=14){const o=Array(close.length).fill(null);let g=0,l=0;for(let i=1;i<close.length;i++){const ch=close[i]-close[i-1],G=ch>0?ch:0,L=ch<0?-ch:0;if(i<=n){g+=G;l+=L;if(i===n){const rs=(g/n)/((l/n)||1e-9);o[i]=100-100/(1+rs)}}else{g=(g*(n-1)+G)/n;l=(l*(n-1)+L)/n;const rs=g/(l||1e-9);o[i]=100-100/(1+rs)}}return o}
function MACD(close,f=12,s=26,sg=9){const ef=EMA(close,f),es=EMA(close,s),m=ef.map((v,i)=>v!=null&&es[i]!=null?v-es[i]:null),signal=EMA(m.map(v=>v??0),sg),h=m.map((v,i)=>v!=null&&signal[i]!=null?v-signal[i]:null);return{macd:m,signal,hist:h}}
function bbBands(close,n=20,k=2){const ma=SMA(close,n),sd=STD(close,n),u=ma.map((m,i)=>m!=null&&sd[i]!=null?m+k*sd[i]:null),l=ma.map((m,i)=>m!=null&&sd[i]!=null?m-k*sd[i]:null);return{ma,upper:u,lower:l}}
function nnum(x){if(x==null)return 0;return +String(x).replace(/,/g,'').replace(/\\s+/g,'').replace(/%/g,'')||0}
const str = (x)=> (x==null ? '' : String(x));
const cumsum = (arr)=>{let s=0; return arr.map(v=>{s += (+v||0); return s;});};
const safeMax = (arr)=> Math.max( ...(arr.map(v=>+v||0).filter(v=>isFinite(v)&&!isNaN(v))), 0 );

function toAsc(date, ...series){
  const N = date.length;
  if (N < 2) return [date, ...series];
  if (date[0] <= date[N-1]) return [date, ...series];
  const rev = a => a.slice().reverse();
  return [rev(date), ...series.map(rev)];
}

function idxOf(cols, primary, alts=[]){
  const i=cols.indexOf(primary);
  if(i>-1) return i;
  for(const a of alts){ const j=cols.indexOf(a); if(j>-1) return j; }
  return -1;
}

function showError(secId,msg){
  for (const side of ['chart-price-','chart-flow-']){
    const el = document.getElementById(side+secId);
    if (el) el.innerHTML = '<div style="padding:12px;color:#b91c1c;font-size:13px">'+msg+'</div>';
  }
}

// ===== ë Œë”ë§ =====
function renderOne(secId){
  const tag=document.getElementById('data-'+secId);
  if(!tag){ showError(secId,'ì„¹ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'); return; }
  let j=null; try{ j=JSON.parse(tag.textContent); }catch(e){ showError(secId,'ì„¹ì…˜ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: '+e); return; }

  const cols=(j.columns||[]).map(c=>String(c).trim());

  const iDate=idxOf(cols,'ì¼ì',['\\ufeffì¼ì','DATE','date']),
        iOpen=idxOf(cols,'ì‹œê°€',['Open','open']),
        iHigh=idxOf(cols,'ê³ ê°€',['High','high']),
        iLow =idxOf(cols,'ì €ê°€',['Low','low']),
        iClose=idxOf(cols,'ì¢…ê°€',['Close','close']),
        iVol =idxOf(cols,'ê±°ë˜ëŸ‰',['Volume','volume']),
        iFor =idxOf(cols,'ì™¸êµ­ì¸ í•©ê³„',['ì™¸êµ­ì¸í•©ê³„','ì™¸ì¸í•©ê³„']),
        iInst=idxOf(cols,'ê¸°ê´€ í•©ê³„',['ê¸°ê´€í•©ê³„']),
        iShortR =idxOf(cols,'ê³µë§¤ë„ë¹„ì¤‘',['ê³µë§¤ë„ ë¹„ì¤‘','ê³µë§¤ë„ ê±°ë˜ëŸ‰ ë¹„ì¤‘','ë¹„ì¤‘','(ê³µë§¤ë„)ë¹„ì¤‘']),
        iShortBR=idxOf(cols,'ê³µë§¤ë„ì”ê³ ë¹„ì¤‘',['ê³µë§¤ë„ ì”ê³  ë¹„ì¤‘','ê³µë§¤ë„ì”ê³ ë¹„ì¤‘(%)','ê³µë§¤ë„ì”ê³  ë¹„ì¤‘(%)','ì”ê³ ë¹„ì¤‘','ì”ê³  ë¹„ì¤‘','ê³µë§¤ë„ì”ê³ ë¹„ìœ¨','ì”ê³ ë¹„ìœ¨']);

  if([iDate,iOpen,iHigh,iLow,iClose].some(i=>i<0)){ showError(secId,'í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½'); return; }
  const rows=j.rows||[]; if(!rows.length){ showError(secId,'ì‹œê³„ì—´ í–‰ì´ ì—†ìŠµë‹ˆë‹¤.'); return; }

  let date   = rows.map(r=>str(r[iDate]));
  let open   = rows.map(r=>nnum(r[iOpen]));
  let high   = rows.map(r=>nnum(r[iHigh]));
  let low    = rows.map(r=>nnum(r[iLow]));
  let close  = rows.map(r=>nnum(r[iClose]));
  let vol    = (iVol>=0)? rows.map(r=>nnum(r[iVol])): rows.map(_=>0);
  let foreign= (iFor>=0)? rows.map(r=>nnum(r[iFor])): rows.map(_=>0);
  let inst   = (iInst>=0)? rows.map(r=>nnum(r[iInst])): rows.map(_=>0);
  let shortR = (iShortR>=0)? rows.map(r=>nnum(r[iShortR])): rows.map(_=>0);
  let shortBR= (iShortBR>=0)? rows.map(r=>nnum(r[iShortBR])): rows.map(_=>0);

  [date, open, high, low, close, vol, foreign, inst, shortR, shortBR] =
    toAsc(date, open, high, low, close, vol, foreign, inst, shortR, shortBR);

  const ma20=SMA(close,20), ma60=SMA(close,60), ma120=SMA(close,120);
  const bb=bbBands(close,20,2);
  const rsi=RSI(close,14);
  const {macd,signal,hist}=MACD(close,12,26,9);

  // ì°¨íŠ¸ 1: ê°€ê²©/ì§€í‘œ
  const layout1={
    grid:{rows:3,columns:1,pattern:'independent',roworder:'top to bottom'},
    xaxis:{domain:[0,1], rangeslider:{visible:false}, showspikes:true, spikemode:'across'},
    yaxis:{domain:[0.35,1.00], title:'ì£¼ê°€ (ì›)', tickformat:',', showspikes:true},
    xaxis2:{anchor:'y2', showspikes:true},
    yaxis2:{domain:[0.18,0.30], title:'RSI', range:[0,100], tickvals:[30,70], showgrid:true},
    xaxis3:{anchor:'y3', showspikes:true},
    yaxis3:{domain:[0.00,0.15], title:'MACD'},
    legend:{orientation:'h', y:1.02, x:0.5, xanchor:'center'},
    margin:{t:40,l:60,r:40,b:30},
    hovermode:'x unified',
    plot_bgcolor:'#ffffff', paper_bgcolor:'#ffffff'
  };

  const traces1=[
    {type:'candlestick',x:date,open,high,low,close,name:'ì£¼ê°€',
     increasing:{line:{color:'#ef4444'}}, decreasing:{line:{color:'#3b82f6'}} },
    {type:'scatter',mode:'lines',x:date,y:ma20,name:'MA20', line:{width:1.5}},
    {type:'scatter',mode:'lines',x:date,y:ma60,name:'MA60', line:{width:1.5}},
    {type:'scatter',mode:'lines',x:date,y:ma120,name:'MA120', line:{width:1.5}},
    {type:'scatter',mode:'lines',x:date,y:bb.upper,name:'BBìƒë‹¨', visible:'legendonly', line:{dash:'dot', width:1}},
    {type:'scatter',mode:'lines',x:date,y:bb.lower,name:'BBí•˜ë‹¨', visible:'legendonly', line:{dash:'dot', width:1}},
    {type:'scatter',mode:'lines',x:date,y:rsi,name:'RSI(14)',xaxis:'x2',yaxis:'y2'},
    {type:'bar',x:date,y:hist,name:'MACD Hist',xaxis:'x3',yaxis:'y3'},
    {type:'scatter',mode:'lines',x:date,y:macd,name:'MACD',xaxis:'x3',yaxis:'y3'},
    {type:'scatter',mode:'lines',x:date,y:signal,name:'Signal',xaxis:'x3',yaxis:'y3'},
  ];

  Plotly.newPlot('chart-price-'+secId, traces1, layout1, {responsive:true, displaylogo:false});

  // ì°¨íŠ¸ 2: ìˆ˜ê¸‰/ê³µë§¤ë„
  const layout2={
    yaxis:{title:'ëˆ„ì  ìˆœë§¤ìˆ˜', tickformat:',', showgrid:true},
    yaxis2:{title:'ê³µë§¤ë„ ë¹„ìœ¨(%)', overlaying:'y', side:'right',
           range:[0, Math.max(1, Math.max(...shortBR, ...shortR, 0)*1.2)]},
    margin:{t:40,l:60,r:50,b:30},
    hovermode:'x unified',
    legend:{orientation:'h', y:1.08, x:0.5, xanchor:'center'},
    plot_bgcolor:'#ffffff'
  };

  const instCum = cumsum(inst);
  const foreignCum = cumsum(foreign);

  const traces2=[
    {type:'scatter',mode:'lines',x:date,y:instCum,   name:'ê¸°ê´€ ëˆ„ì '},
    {type:'scatter',mode:'lines',x:date,y:foreignCum,name:'ì™¸êµ­ì¸ ëˆ„ì '},
    {type:'scatter',mode:'lines',x:date,y:shortR,    name:'ê³µë§¤ë„ë¹„ì¤‘(%)',yaxis:'y2', line:{dash:'dot'}},
    {type:'scatter',mode:'lines',x:date,y:shortBR,   name:'ê³µë§¤ë„ì”ê³ ë¹„ì¤‘(%)',yaxis:'y2'},
  ];

  Plotly.newPlot('chart-flow-'+secId, traces2, layout2, {responsive:true, displaylogo:false});
}

(function main(){
  const ids=Array.from(document.querySelectorAll('section[id]')).map(s=>s.id);
  for(const id of ids){ try{ renderOne(id); }catch(e){ showError(id,'ë Œë”ë§ ì˜¤ë¥˜: '+e); } }
})();
</script>

<footer style="margin-top:40px;color:#666;font-size:12px">
  Published via GitHub Pages Â· Per-ticker JSON: /api/*.json
</footer>
</body>
</html>""")

    html_doc = html_template.substitute(
        generated=generated,
        nav=nav,
        sections="".join(sections) if sections else "<p>í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>",
    )

    (docs_dir / "index.html").write_text(html_doc, encoding="utf-8")
    logging.info("index.html ìƒì„± ì™„ë£Œ â†’ %s", docs_dir / "index.html")

# =========================
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# =========================
def main():
    parser = argparse.ArgumentParser(description="KRX ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ & CSV ì—…ë°ì´íŠ¸")
    parser.add_argument("--company-list", default=str(DATA_DIR / "company_list.txt"))
    parser.add_argument("--run-on-holiday", default="true", help="íœ´ì¥ì¼ì—ë„ ì‹¤í–‰ (true/false)")
    parser.add_argument("--rows-limit", default=os.getenv("ROWS_LIMIT", "").strip(),
                        help="HTML/JSON í¬í•¨ ìµœëŒ€ í–‰ ìˆ˜ (ë¹ˆ ê°’ì´ë©´ ì „ëŸ‰)")
    args = parser.parse_args()

    logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")

    run_on_holiday = str(args.run_on_holiday).lower() in ("1","true","yes","y")
    rows_limit = None if args.rows_limit in ("", "0", "none", "None") else int(args.rows_limit)

    try:
        companies = read_company_list(Path(args.company_list))
    except Exception as e:
        logging.exception("ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨: %s", e)
        return

    if not companies:
        logging.warning("ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    changed = False
    for name, ticker in companies:
        try:
            time.sleep(SLEEP_SEC)
            updated = upsert_company(name, ticker, run_on_holiday)
            changed = changed or updated
        except Exception as e:
            logging.exception("[%s,%s] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: %s", name, ticker, e)

    if changed:
        logging.info("ë³€ê²½ì‚¬í•­ ì¡´ì¬ â†’ ì»¤ë°‹ ë‹¨ê³„ì—ì„œ ë°˜ì˜ë©ë‹ˆë‹¤.")
    else:
        logging.info("ë³€ê²½ì‚¬í•­ ì—†ìŒ.")

    # ë‹¨ì¼ index.jsonì€ ë§Œë“¤ì§€ ì•ŠìŒ â†’ ê¸°ì—…ë³„ JSON + index.htmlë§Œ ìƒì„±
    emit_per_ticker_json(companies, rows_limit=rows_limit)
    emit_index_html(companies, rows_limit=rows_limit)

if __name__ == "__main__":
    main()
